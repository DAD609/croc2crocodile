{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rTjpmQdgzYKb"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers torch\n",
        "!pip install python-docx\n",
        "!pip install --upgrade jupyter ipywidgets\n",
        "!pip install pymilvus\n",
        "!pip install pymongo\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install pymongo\n",
        "!pip install 'pymongo[srv]'\n",
        "!pip install language-tool-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoRmPkAJoNbz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEiuvllUq-Am"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure\n",
        "\n",
        "uri = \"mongodb+srv://dad:dbdad69@croc.qixoyll.mongodb.net/?retryWrites=true&w=majority&appName=croc\"\n",
        "\n",
        "client = MongoClient(uri)\n",
        "\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except ConnectionFailure as e:\n",
        "    print(\"Failed to connect to MongoDB:\", e)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb4WfyrZyIrL"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "import os\n",
        "uri = \"mongodb+srv://dad:dbdad69@croc.qixoyll.mongodb.net/?retryWrites=true&w=majority&appName=croc\"\n",
        "client = MongoClient(uri)\n",
        "db = client['doc2doc']\n",
        "text_collection = db['text_documents']\n",
        "\n",
        "def upload_text_documents(directory, collection):\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if filename.endswith('.docx'):\n",
        "            doc = Document(filepath)\n",
        "            full_text = []\n",
        "            for para in doc.paragraphs:\n",
        "                full_text.append(para.text)\n",
        "            full_text = '\\n'.join(full_text)\n",
        "            document = {\n",
        "                'filename': filename,\n",
        "                'content': full_text\n",
        "            }\n",
        "            collection.insert_one(document)\n",
        "            print(f\"Uploaded text from {filename} to MongoDB.\")\n",
        "upload_documents('/content/drive/My Drive/Colab Notebooks/плохо', bad_collection)\n",
        "upload_documents('/content/drive/My Drive/Colab Notebooks/хорошо', good_collection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPkiI_rHllw0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from docx import Document\n",
        "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "logging.basicConfig(filename='training_logs.txt', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "def extract_text(doc_path):\n",
        "    logging.info(f\"Извлечение документов {doc_path}\")\n",
        "    doc = Document(doc_path)\n",
        "    return [para.text for para in doc.paragraphs if para.text.strip()]\n",
        "\n",
        "def create_dataset(directory_bad, directory_good, tokenizer):\n",
        "    logging.info(\"Объявление датасета\")\n",
        "    data_entries = []\n",
        "    bad_files = sorted([f for f in os.listdir(directory_bad) if f.endswith('.docx')])\n",
        "    good_files = sorted([f for f in os.listdir(directory_good) if f.endswith('.docx')])\n",
        "    for bad_file, good_file in zip(bad_files, good_files):\n",
        "        bad_texts = extract_text(os.path.join(directory_bad, bad_file))\n",
        "        good_texts = extract_text(os.path.join(directory_good, good_file))\n",
        "        for bad_text, good_text in zip(bad_texts, good_texts):\n",
        "            tokenized_input = tokenizer(bad_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            tokenized_target = tokenizer(good_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            data_entries.append({\n",
        "                'input_ids': tokenized_input['input_ids'].squeeze(),\n",
        "                'attention_mask': tokenized_input['attention_mask'].squeeze(),\n",
        "                'labels': tokenized_target['input_ids'].squeeze()\n",
        "            })\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': [entry['input_ids'] for entry in data_entries],\n",
        "        'attention_mask': [entry['attention_mask'] for entry in data_entries],\n",
        "        'labels': [entry['labels'] for entry in data_entries]\n",
        "    })\n",
        "\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\").to('cuda')\n",
        "\n",
        "bad_directory = '/content/drive/My Drive/Colab Notebooks/плохо'\n",
        "good_directory = '/content/drive/My Drive/Colab Notebooks/хорошо'\n",
        "dataset = create_dataset(bad_directory, good_directory, tokenizer)\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['test']\n",
        ")\n",
        "\n",
        "logging.info(\"Начало обучения\")\n",
        "trainer.train()\n",
        "logging.info(\"Умная модель готова\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWUI3xs84iIm"
      },
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RfIf7NahZz6D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from docx import Document\n",
        "from pymongo import MongoClient\n",
        "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
        "import concurrent.futures\n",
        "import language_tool_python\n",
        "\n",
        "logging.basicConfig(filename='generation_logs.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "uri = \"mongodb+srv://dad:dbdad69@croc.qixoyll.mongodb.net/?retryWrites=true&w=majority&appName=croc\"\n",
        "client = MongoClient(uri)\n",
        "db = client['doc2doc']\n",
        "collection = db['stiliz']\n",
        "\n",
        "\n",
        "model_name = \"google/mt5-small\"\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.to('cuda')\n",
        "\n",
        "tool = language_tool_python.LanguageTool('ru-RU')\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'<extra_id_\\d+>', '', text).strip()\n",
        "\n",
        "def generate_improved_text(text):\n",
        "    logging.info(f\"Начало обработки текста: {text[:30]}...\")\n",
        "    input_ids = tokenizer.encode(\"improve text: \" + text, return_tensors=\"pt\").to('cuda')\n",
        "    generated_ids = model.generate(input_ids, max_length=512)\n",
        "    improved_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    # Очистка текста и исправление грамматических ошибок\n",
        "    improved_text = clean_text(improved_text)\n",
        "    matches = tool.check(improved_text)\n",
        "    corrected_text = language_tool_python.utils.correct(improved_text, matches)\n",
        "    logging.info(\"Конец генерации\")\n",
        "    return corrected_text(improved_text)\n",
        "\n",
        "def process_document(document):\n",
        "    new_doc = Document()\n",
        "    for para_data in document['content']:\n",
        "        new_para = new_doc.add_paragraph(generate_improved_text(para_data['text']))\n",
        "        for run_data in para_data['styles']:\n",
        "            new_run = new_para.add_run(run_data['text'])\n",
        "            new_run.bold = run_data.get('bold', False)\n",
        "            new_run.italic = run_data.get('italic', False)\n",
        "            new_run.underline = run_data.get('underline', False)\n",
        "            if run_data.get('font_name'):\n",
        "                new_run.font.name = run_data['font_name']\n",
        "            if run_data.get('font_size'):\n",
        "                new_run.font.size = Pt(run_data['font_size'])\n",
        "    return new_doc\n",
        "\n",
        "documents = list(collection.find())\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    new_docs = list(executor.map(process_document, documents))\n",
        "\n",
        "for idx, doc in enumerate(new_docs):\n",
        "    doc_path = f'/content/drive/My Drive/Colab Notebooks/updated_{idx}.docx'\n",
        "    doc.save(doc_path)\n",
        "    logging.info(f\"Файл {doc_path} успешно создан\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}