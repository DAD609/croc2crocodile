{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rTjpmQdgzYKb"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers torch\n",
        "!pip install python-docx\n",
        "!pip install --upgrade jupyter ipywidgets\n",
        "!pip install pymilvus\n",
        "!pip install pymongo\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install pymongo\n",
        "!pip install 'pymongo[srv]'\n",
        "!pip install language-tool-python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GoRmPkAJoNbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from docx import Document\n",
        "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "logging.basicConfig(filename='training_logs.txt', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "def extract_text(doc_path):\n",
        "    logging.info(f\"Извлечение документов {doc_path}\")\n",
        "    doc = Document(doc_path)\n",
        "    return [para.text for para in doc.paragraphs if para.text.strip()]\n",
        "\n",
        "def create_dataset(directory_bad, directory_good, tokenizer):\n",
        "    logging.info(\"Объявление датасета\")\n",
        "    data_entries = []\n",
        "    bad_files = sorted([f for f in os.listdir(directory_bad) if f.endswith('.docx')])\n",
        "    good_files = sorted([f for f in os.listdir(directory_good) if f.endswith('.docx')])\n",
        "    for bad_file, good_file in zip(bad_files, good_files):\n",
        "        bad_texts = extract_text(os.path.join(directory_bad, bad_file))\n",
        "        good_texts = extract_text(os.path.join(directory_good, good_file))\n",
        "        for bad_text, good_text in zip(bad_texts, good_texts):\n",
        "            tokenized_input = tokenizer(bad_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            tokenized_target = tokenizer(good_text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            data_entries.append({\n",
        "                'input_ids': tokenized_input['input_ids'].squeeze(),\n",
        "                'attention_mask': tokenized_input['attention_mask'].squeeze(),\n",
        "                'labels': tokenized_target['input_ids'].squeeze()\n",
        "            })\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': [entry['input_ids'] for entry in data_entries],\n",
        "        'attention_mask': [entry['attention_mask'] for entry in data_entries],\n",
        "        'labels': [entry['labels'] for entry in data_entries]\n",
        "    })\n",
        "\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\").to('cuda')\n",
        "\n",
        "bad_directory = '/content/drive/My Drive/Colab Notebooks/плохо'\n",
        "good_directory = '/content/drive/My Drive/Colab Notebooks/хорошо'\n",
        "dataset = create_dataset(bad_directory, good_directory, tokenizer)\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['test']\n",
        ")\n",
        "\n",
        "logging.info(\"Начало обучения\")\n",
        "trainer.train()\n",
        "logging.info(\"Умная модель готова\")"
      ],
      "metadata": {
        "id": "VPkiI_rHllw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "HWUI3xs84iIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import re\n",
        "from docx import Document\n",
        "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
        "import concurrent.futures\n",
        "import language_tool_python\n",
        "\n",
        "logging.basicConfig(filename='generation_logs.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "model_name = \"google/mt5-small\"\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model.to('cuda')\n",
        "\n",
        "tool = language_tool_python.LanguageTool('ru-RU')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<extra_id_\\d+>', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def generate_improved_text(text):\n",
        "    logging.info(f\"Начало обработки текста: {text[:30]}...\")\n",
        "    input_ids = tokenizer.encode(\"improve text: \" + text, return_tensors=\"pt\").to('cuda')\n",
        "    generated_ids = model.generate(input_ids, max_length=512)\n",
        "    improved_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    # Очистка текста и исправление грамматических ошибок\n",
        "    improved_text = clean_text(improved_text)\n",
        "    matches = tool.check(improved_text)\n",
        "    corrected_text = language_tool_python.utils.correct(improved_text, matches)\n",
        "    logging.info(\"Конец генерации\")\n",
        "    return corrected_text\n",
        "\n",
        "def process_paragraph(text):\n",
        "    return generate_improved_text(text) if text.strip() else \"\"\n",
        "\n",
        "doc_path = '/content/drive/My Drive/Colab Notebooks/elibrary_37083625_67327706.docx'\n",
        "doc = Document(doc_path)\n",
        "new_doc = Document()\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = list(executor.map(process_paragraph, [para.text for para in doc.paragraphs]))\n",
        "\n",
        "for result, para in zip(results, doc.paragraphs):\n",
        "    new_para = new_doc.add_paragraph(result)\n",
        "    for run in para.runs:\n",
        "        new_run = new_para.add_run(run.text)\n",
        "        new_run.bold = run.bold\n",
        "        new_run.italic = run.italic\n",
        "        new_run.underline = run.underline\n",
        "        if run.font.size:\n",
        "            new_run.font.size = run.font.size\n",
        "        if run.font.name:\n",
        "            new_run.font.name = run.font.name\n",
        "\n",
        "new_doc_path = '/content/drive/My Drive/Colab Notebooks/24updated_elibrary_37083625_67327706.docx'\n",
        "new_doc.save(new_doc_path)\n",
        "logging.info(\"Файл успешно создан\")\n"
      ],
      "metadata": {
        "id": "RfIf7NahZz6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}