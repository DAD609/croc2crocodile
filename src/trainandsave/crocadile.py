# -*- coding: utf-8 -*-
"""crocadile.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzlxn5VA14FYGnARDBvDZgQ-mQOngLM_
"""

!pip install transformers
!pip install transformers torch
!pip install python-docx
!pip install --upgrade jupyter ipywidgets
!pip install pymilvus
!pip install pymongo
!pip install datasets
!pip install accelerate -U
!pip install pymongo
!pip install 'pymongo[srv]'

from google.colab import drive
drive.mount('/content/drive')

import os
import logging
from docx import Document
from transformers import MT5Tokenizer, MT5ForConditionalGeneration, TrainingArguments, Trainer
from datasets import Dataset, DatasetDict

logging.basicConfig(filename='training_logs.txt', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')

def extract_text(doc_path):
    logging.info(f"Извлечение документов {doc_path}")
    doc = Document(doc_path)
    return [para.text for para in doc.paragraphs if para.text.strip()]

def create_dataset(directory_bad, directory_good, tokenizer):
    logging.info("Объявление датасета")
    data_entries = []
    bad_files = sorted([f for f in os.listdir(directory_bad) if f.endswith('.docx')])
    good_files = sorted([f for f in os.listdir(directory_good) if f.endswith('.docx')])
    for bad_file, good_file in zip(bad_files, good_files):
        bad_texts = extract_text(os.path.join(directory_bad, bad_file))
        good_texts = extract_text(os.path.join(directory_good, good_file))
        for bad_text, good_text in zip(bad_texts, good_texts):
            tokenized_input = tokenizer(bad_text, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
            tokenized_target = tokenizer(good_text, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
            data_entries.append({
                'input_ids': tokenized_input['input_ids'].squeeze(),
                'attention_mask': tokenized_input['attention_mask'].squeeze(),
                'labels': tokenized_target['input_ids'].squeeze()
            })
    return Dataset.from_dict({
        'input_ids': [entry['input_ids'] for entry in data_entries],
        'attention_mask': [entry['attention_mask'] for entry in data_entries],
        'labels': [entry['labels'] for entry in data_entries]
    })

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small").to('cuda')

bad_directory = '/content/drive/My Drive/Colab Notebooks/плохо'
good_directory = '/content/drive/My Drive/Colab Notebooks/хорошо'
dataset = create_dataset(bad_directory, good_directory, tokenizer)
train_test_split = dataset.train_test_split(test_size=0.1)
dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=4,
    logging_dir='./logs',
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_dict['train'],
    eval_dataset=dataset_dict['test']
)

logging.info("Начало обучения")
trainer.train()
logging.info("Умная модель готова")
